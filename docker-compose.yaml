services:
  agent:
    build:
      context: .
      platforms:
        - "linux/amd64"
    platform: "linux/amd64"
    environment:
      - OLLAMA_BASE_API_URL=http://ollama:11434
      - HF_TOKEN=
    networks:
      - app
    depends_on:
      - ollama
#    Enable GPU support using host machine
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [ gpu ]

  ollama:
    image: ollama/ollama:${OLLAMA_TAG:-latest}
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - app
    restart: always

  ollama-setup:
    image: curlimages/curl:${OLLAMA_TAG:-latest}
    command:
      - -X
      - POST
      - http://ollama:11434/api/pull
      - -d
      - '{"name": "mistral:7b"}' # mistral:instruct OR llama3:instruct OR phi3:instruct OR
      - -H
      - "Content-Type: application/json"
    networks:
      - app
    depends_on:
      - ollama

volumes:
  ollama:

networks:
  app: